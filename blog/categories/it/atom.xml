<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: it | Marcin bloguje]]></title>
  <link href="http://zygm0nt.github.com/blog/categories/it/atom.xml" rel="self"/>
  <link href="http://zygm0nt.github.com/"/>
  <updated>2013-12-08T22:19:52+01:00</updated>
  <id>http://zygm0nt.github.com/</id>
  <author>
    <name><![CDATA[Marcin Cylke]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Simple HBase ORM]]></title>
    <link href="http://zygm0nt.github.com/blog/2013/12/08/simple-hbase-orm/"/>
    <updated>2013-12-08T21:08:00+01:00</updated>
    <id>http://zygm0nt.github.com/blog/2013/12/08/simple-hbase-orm</id>
    <content type="html"><![CDATA[<p>When dealing with data stored in HBase, you are quick to come to a
conclusion, that it is extremaly inconvenient to reach to it
via HBase native API. It is very verbose and you always need to convert
between bytes and simple types - a pain.</p>

<p>While I was working on a project of mine, I thought, why not to easy
those pains and fetch real objects from HBase.</p>

<p>And that's how this simplistic, hackish ORM came to life. It is no match
for projects like <a href="https://github.com/impetus-opensource/Kundera">Kundera</a>
(a JPA compliant solution), or <a href="https://code.google.com/p/n-orm/">n-orm</a>. Nevertheless, it just suits my needs :)</p>

<!-- more -->


<p>Project sources are hosted on GitHub: <a href="https://github.com/zygm0nt/hbase-annotations">https://github.com/zygm0nt/hbase-annotations</a></p>

<p>To make use of this, you need to have an entity class with annotations:</p>

<ul>
<li>@Column - with argument specifying column family and column name, ie.
@Column("cf:column-name")</li>
<li>@Id - will store row key in this property,</li>
<li>and optionaly @Value - for Spring Expression Language, in case you
need to perform some extraction on the value.</li>
</ul>


<p><em>Annotations should be on setter methods.</em></p>

<p>Now you have your model annotated and ready to be fetched from HBase.</p>

<p>The actual work is done with a service class, that should extend class
<a href="https://github.com/zygm0nt/hbase-annotations/blob/master/src/main/java/pl/touk/hadoop/hbase/BaseHadoopInteraction.java">BaseHadoopInteraction</a> just as class
<a href="https://github.com/zygm0nt/hbase-annotations/blob/master/src/test/java/pl/touk/hadoop/hbase/SampleHBaseClient.java">SimpleHBaseClient</a> does.</p>

<p>Then it is possible to just call:</p>

<script src="https://gist.github.com/zygm0nt/7863407.js"></script>


<p>Note that there are more methods you can use on BaseHadoopInteraction.
You can also do:</p>

<ul>
<li>scan</li>
<li>scan with key ranges</li>
<li>delete</li>
</ul>


<p>What you won't get from this simple ORM is:</p>

<ul>
<li>automatic object updating,</li>
<li>nested objects,</li>
<li>saving to HBase - 'cause I didn't have a need for that,</li>
</ul>


<p>Hope you'll find this piece of code useful. If you see room for
improvements while staying in project's scope - please drop me a
message.</p>

<p>And if you are searching for a full-fledged ORM solution for interacting with HBase, just head
straight to Kundera project website :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Recently at storm-users]]></title>
    <link href="http://zygm0nt.github.com/blog/2013/08/12/recently-at-storm-users/"/>
    <updated>2013-08-12T22:26:00+02:00</updated>
    <id>http://zygm0nt.github.com/blog/2013/08/12/recently-at-storm-users</id>
    <content type="html"><![CDATA[<p>I've been reading through storm-users Google Group recently. This
resolution was heavily inspired by Adam Kawa's post <a href="http://hakunamapdata.com/football-zero-apache-pig-hero-the-essence-from-hundreds-of-posts-from-apache-pig-user-mailing-list/">"Football zero, Apache Pig hero"</a>. Since I've encountered a lot of insightful and very interesting information I've decided to describe some of those in this post.</p>

<!-- more -->


<ul>
<li><p>nimbus will work in HA mode - There's a pull request open for it already... but some
recent work (distributing topology files via Bittorrent) will greatly
simplify the implementation. Once the Bittorrent work is done we'll look
at reworking the HA pull request. (<a href="https://github.com/nathanmarz/storm/pull/629">storm’s pull request</a>)</p></li>
<li><p>pig on storm - Pig on Trident would be a cool and welcome project. Join
and groupBy have very clear semantics there, as those concepts exist
directly in Trident. The extensions needed to Pig are the concept of
incremental, persistent state across batches (mirroring those concepts
in Trident). You can read a complete <a href="https://cwiki.apache.org/confluence/display/PIG/Pig+on+Storm+Proposal">proposal</a>.</p></li>
<li><p>implementing topologies in pure python with <a href="https://github.com/AirSage/Petrel">petrel</a> looks like this:</p></li>
</ul>


<blockquote><pre><code>class Bolt(storm.BasicBolt):
    def initialize(self, conf, context):
       ''' This method executed only once '''
        storm.log('initializing bolt')

    def process(self, tup):
       ''' This method executed every time a new tuple arrived '''       
       msg = tup.values[0]
       storm.log('Got tuple %s' %msg)

if __name__ == "__main__":
    Bolt().run()
</code></pre></blockquote>

<ul>
<li><p>Fliptop is happy with storm - see their presentation <a href="http://www.slideshare.net/robbiecheng/lesson-learned-of-twitter-storm">here</a></p></li>
<li><p>topology metrics in 0.9.0: The new metrics feature allows you to collect
arbitrarily custom metrics over fixed windows. Those metrics are
exported to a metrics stream that you can consume by implementing
<a href="https://github.com/nathanmarz/storm/blob/master/storm-core/src/jvm/backtype/storm/metric/api/IMetricsConsumer.java">IMetricsConsumer</a> and configure with <a href="https://github.com/nathanmarz/storm/blob/master/storm-core/src/jvm/backtype/storm/Config.java#L473">Config.java#L473</a>.
Use <strong>TopologyContext#registerMetric</strong> to register new metrics.</p></li>
<li><p>storm vs flume - some users' point of view: I use Storm and Flume and find that they are better at
different things - it really depends on your use case as to which one is
better suited. First and foremost, they were originally designed to do
different things: Flume is a reliable service for collecting,
aggregating, and moving large amounts of data from source to destination
(e.g. log data from many web servers to HDFS). Storm is more for
real-time computation (e.g. streaming analytics) where you analyse data
in flight and don't necessarily land it anywhere. Having said that,
Storm is also fault-tolerant and can write to external data stores (e.g.
HBase) and you can do real-time computation in Flume (using
interceptors)</p></li>
</ul>


<p>That's all for this day - however, I'll keep on reading through storm-users, so watch this space for more info on storm development.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[After WHUG meeting]]></title>
    <link href="http://zygm0nt.github.com/blog/2012/11/30/after-whug/"/>
    <updated>2012-11-30T22:20:00+01:00</updated>
    <id>http://zygm0nt.github.com/blog/2012/11/30/after-whug</id>
    <content type="html"><![CDATA[<p>Here are the slides from the talk a gave yesterday. If you have any
questions, please ask.</p>

<script async class="speakerdeck-embed"
data-id="cc18d5601d60013059a31231381554d7" data-ratio="1.33333333333333"
src="http://zygm0nt.github.com//speakerdeck.com/assets/embed.js"></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WHUG 8. Beyond Hadoop - checking other options]]></title>
    <link href="http://zygm0nt.github.com/blog/2012/11/26/whug-8-beyond-hadoop/"/>
    <updated>2012-11-26T09:11:00+01:00</updated>
    <id>http://zygm0nt.github.com/blog/2012/11/26/whug-8-beyond-hadoop</id>
    <content type="html"><![CDATA[<p>W najbliższy czwartek - czyli 29.11.2012 - poprowadzę prezentację w
ramach Warsaw Hadoop User Group. Swoją obecność można odklinąć tu
http://www.meetup.com/warsaw-hug/</p>

<p>A o czym będę mówił? Przeklejka ze strony WHUG:</p>

<blockquote><p>Marcin skupi się na współpracy ekosystemu Hadoopa z innymi narzędziami.
Pokaże jak prosto i wygodnie przetwarzać grafy i jak stosować podejście
Big Data, w czasie rzeczywistym. Poruszy również temat łatwiejszego
tworzenia algorytmów Map-Reduce</p>

<p>Będzie to nieco mniej technicza (ale wciąż praktyczna) wycieczka po
obrzeżach tematyki, która jest zwykle poruszana w połączeniu z
Hadoop-em.</p>

<p>Prezentacja będzie dotyczyć narzędzi takich jak Cascading, Storm, Titan.</p></blockquote>

<p>Zapraszam!</p>
]]></content>
  </entry>
  
</feed>
