<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hadoop on Tech ramblings by Marcin</title>
    <link>http://marcin.cylke.com.pl/categories/hadoop/</link>
    <description>Recent content in hadoop on Tech ramblings by Marcin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Dec 2013 00:00:00 +0000</lastBuildDate><atom:link href="http://marcin.cylke.com.pl/categories/hadoop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Distributed scans with HBase</title>
      <link>http://marcin.cylke.com.pl/2013/12/10/distributed-scans-with-hbase/</link>
      <pubDate>Tue, 10 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>http://marcin.cylke.com.pl/2013/12/10/distributed-scans-with-hbase/</guid>
      <description>&lt;p&gt;HBase is by design a columnar store, that is optimized for random reads.
You just ask for a row using rowId as an identifier and you get your
data instantaneously.&lt;/p&gt;
&lt;p&gt;Performing a scan on part or whole table is a completely different thing.
First of all, it is sequential. Meaning it is rather slow, because it
doesn&amp;rsquo;t use all the RegionServers at the same time. It is implemented
that way to realize the contract of Scan command - which has to return
results sorted by key.&lt;/p&gt;
&lt;p&gt;So, how to do this efficiently?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Simple HBase ORM</title>
      <link>http://marcin.cylke.com.pl/2013/12/08/simple-hbase-orm/</link>
      <pubDate>Sun, 08 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>http://marcin.cylke.com.pl/2013/12/08/simple-hbase-orm/</guid>
      <description>&lt;p&gt;When dealing with data stored in HBase, you are quick to come to a
conclusion, that it is extremaly inconvenient to reach to it
via HBase native API. It is very verbose and you always need to convert
between bytes and simple types - a pain.&lt;/p&gt;
&lt;p&gt;While I was working on a project of mine, I thought, why not to easy
those pains and fetch real objects from HBase.&lt;/p&gt;
&lt;p&gt;And that&amp;rsquo;s how this simplistic, hackish ORM came to life. It is no match
for projects like &lt;a href=&#34;https://github.com/impetus-opensource/Kundera&#34;&gt;Kundera&lt;/a&gt;
(a JPA compliant solution), or &lt;a href=&#34;https://code.google.com/p/n-orm/&#34;&gt;n-orm&lt;/a&gt;. Nevertheless, it just suits my needs :)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Zookeeper &#43; Curator = Distributed sync</title>
      <link>http://marcin.cylke.com.pl/2013/06/24/zookeeper-curator/</link>
      <pubDate>Mon, 24 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>http://marcin.cylke.com.pl/2013/06/24/zookeeper-curator/</guid>
      <description>An application developed for one of my recent projects at TouK involved multiple servers. There was a requirement to ensure failover for the system&amp;rsquo;s components. Since I had already a few separate components I didn&amp;rsquo;t want to add more of that, and since there already was a Zookeeper ensemble running - required by one of the services, I&amp;rsquo;ve decided to go that way with my solution. What is Zookeeper?</description>
    </item>
    
    <item>
      <title>Operational problems with Zookeeper</title>
      <link>http://marcin.cylke.com.pl/2013/03/21/zookeeper-tips/</link>
      <pubDate>Thu, 21 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>http://marcin.cylke.com.pl/2013/03/21/zookeeper-tips/</guid>
      <description>This post is a summary of what has been presented by Kathleen Ting on StrangeLoop conference. You can watch the original here: http://www.infoq.com/presentations/Misconfiguration-ZooKeeper
I&amp;rsquo;ve decided to put this selection here for quick reference.
Connection mismanagement   too many connections
 WARN [NIOServerCxn.Factory: 0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@247] - Too many connections from /xx.x.xx.xxx - max is 60     running out of ZK connections?  set maxClientCnxns=200 in zoo.cfg   HBase client leaking connections?</description>
    </item>
    
    <item>
      <title>After WHUG meeting</title>
      <link>http://marcin.cylke.com.pl/2012/11/30/after-whug/</link>
      <pubDate>Fri, 30 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>http://marcin.cylke.com.pl/2012/11/30/after-whug/</guid>
      <description>Here are the slides from the talk a gave yesterday. If you have any questions, please ask.</description>
    </item>
    
    <item>
      <title>WHUG 8. Beyond Hadoop - checking other options</title>
      <link>http://marcin.cylke.com.pl/2012/11/26/whug-8-beyond-hadoop/</link>
      <pubDate>Mon, 26 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>http://marcin.cylke.com.pl/2012/11/26/whug-8-beyond-hadoop/</guid>
      <description>W najbliższy czwartek - czyli 29.11.2012 - poprowadzę prezentację w ramach Warsaw Hadoop User Group. Swoją obecność można odklinąć tu http://www.meetup.com/warsaw-hug/
A o czym będę mówił? Przeklejka ze strony WHUG:
 Marcin skupi się na współpracy ekosystemu Hadoopa z innymi narzędziami. Pokaże jak prosto i wygodnie przetwarzać grafy i jak stosować podejście Big Data, w czasie rzeczywistym. Poruszy również temat łatwiejszego tworzenia algorytmów Map-Reduce
Będzie to nieco mniej technicza (ale wciąż praktyczna) wycieczka po obrzeżach tematyki, która jest zwykle poruszana w połączeniu z Hadoop-em.</description>
    </item>
    
    <item>
      <title>Hadoop HA setup</title>
      <link>http://marcin.cylke.com.pl/2012/10/30/hadoop-ha/</link>
      <pubDate>Tue, 30 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>http://marcin.cylke.com.pl/2012/10/30/hadoop-ha/</guid>
      <description>&lt;p&gt;With the advent of Hadoop&amp;rsquo;s 2.x version, there finally is a working
High-Availability solution. Even two of those. Now it really is easy to
configure and use those solutions. It no longer require external
components, like
&lt;a href=&#34;http://blog.cloudera.com/blog/2009/07/hadoop-ha-configuration/&#34;&gt;DRBD&lt;/a&gt;.
It all is just neatly packed into Cloudera Hadoop distribution - the
precursor of this solution.&lt;/p&gt;
&lt;p&gt;Read on to find out how to use it.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
