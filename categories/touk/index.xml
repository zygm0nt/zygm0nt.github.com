<?xml version="1.0" encoding="utf-8" standalone="yes"?><?xml-stylesheet href="/feed.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Touk on Tech ramblings by Marcin</title>
    <link>https://marcin.cylke.com.pl/categories/touk/</link>
    <description>Recent content in Touk on Tech ramblings by Marcin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Jul 2014 00:00:00 +0000</lastBuildDate><atom:link href="https://marcin.cylke.com.pl/categories/touk/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Super Confitura Man</title>
      <link>https://marcin.cylke.com.pl/2014/07/14/super-confitura-man/</link>
      <pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://marcin.cylke.com.pl/2014/07/14/super-confitura-man/</guid>
      <description>&lt;h2 id=&#34;how-super-confitura-man-came-to-be-&#34;&gt;How Super Confitura Man came to be :)&lt;/h2&gt;
&lt;p&gt;Recently at TouK we had a one-day hackathon. There was no main theme for
it, you just could post a project idea, gather people around it and hack
on that idea for a whole day - drinks and pizza included.&lt;/p&gt;
&lt;p&gt;My main idea was to create something that could be fun to build and be
useful somehow to others. I’d figured out that since Confitura was just
around a corner I could make a game, that would be playable at TouK’s
booth at the conference venue. This idea seemed good enough to attract
Rafał Nowak &lt;a href=&#34;https://twitter.com/RNowak3&#34;&gt;@RNowak3&lt;/a&gt; and Marcin Jasion
&lt;a href=&#34;https://twitter.com/marcinjasion&#34;&gt;@marcinjasion&lt;/a&gt; - two TouK employees, that with me
formed a team for the hackathon.&lt;/p&gt;
&lt;p&gt;















  
  
      
      
  &lt;picture&gt;
  &lt;img class=&#34;img-fluid&#34; src=&#34;https://marcin.cylke.com.pl/assets/confitura-2014-01.jpg?v=06b2b14756d696ffa222e69e68d540d5&#34; alt=&#34;Confitura 01&#34; loading=&#34;lazy&#34; height=&#34;576&#34; width=&#34;1024&#34; /&gt;
&lt;/picture&gt;
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Distributed scans with HBase</title>
      <link>https://marcin.cylke.com.pl/2013/12/10/distributed-scans-with-hbase/</link>
      <pubDate>Tue, 10 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://marcin.cylke.com.pl/2013/12/10/distributed-scans-with-hbase/</guid>
      <description>&lt;p&gt;HBase is by design a columnar store, that is optimized for random reads.
You just ask for a row using rowId as an identifier and you get your
data instantaneously.&lt;/p&gt;
&lt;p&gt;Performing a scan on part or whole table is a completely different thing.
First of all, it is sequential. Meaning it is rather slow, because it
doesn&amp;rsquo;t use all the RegionServers at the same time. It is implemented
that way to realize the contract of Scan command - which has to return
results sorted by key.&lt;/p&gt;
&lt;p&gt;So, how to do this efficiently?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Simple HBase ORM</title>
      <link>https://marcin.cylke.com.pl/2013/12/08/simple-hbase-orm/</link>
      <pubDate>Sun, 08 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://marcin.cylke.com.pl/2013/12/08/simple-hbase-orm/</guid>
      <description>&lt;p&gt;When dealing with data stored in HBase, you are quick to come to a
conclusion, that it is extremaly inconvenient to reach to it
via HBase native API. It is very verbose and you always need to convert
between bytes and simple types - a pain.&lt;/p&gt;
&lt;p&gt;While I was working on a project of mine, I thought, why not to easy
those pains and fetch real objects from HBase.&lt;/p&gt;
&lt;p&gt;And that&amp;rsquo;s how this simplistic, hackish ORM came to life. It is no match
for projects like &lt;a href=&#34;https://github.com/impetus-opensource/Kundera&#34;&gt;Kundera&lt;/a&gt;
(a JPA compliant solution), or &lt;a href=&#34;https://code.google.com/p/n-orm/&#34;&gt;n-orm&lt;/a&gt;. Nevertheless, it just suits my needs :)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Recently at storm-users</title>
      <link>https://marcin.cylke.com.pl/2013/08/12/recently-at-storm-users/</link>
      <pubDate>Mon, 12 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://marcin.cylke.com.pl/2013/08/12/recently-at-storm-users/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been reading through storm-users Google Group recently. This
resolution was heavily inspired by Adam Kawa&amp;rsquo;s post &lt;a href=&#34;http://hakunamapdata.com/football-zero-apache-pig-hero-the-essence-from-hundreds-of-posts-from-apache-pig-user-mailing-list/&#34;&gt;&amp;ldquo;Football zero, Apache Pig hero&amp;rdquo;&lt;/a&gt;. Since I&amp;rsquo;ve encountered a lot of insightful and very interesting information I&amp;rsquo;ve decided to describe some of those in this post.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Zookeeper &#43; Curator = Distributed sync</title>
      <link>https://marcin.cylke.com.pl/2013/06/24/zookeeper-curator/</link>
      <pubDate>Mon, 24 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://marcin.cylke.com.pl/2013/06/24/zookeeper-curator/</guid>
      <description>An application developed for one of my recent projects at TouK involved multiple servers. There was a requirement to ensure failover for the system&amp;rsquo;s components. Since I had already a few separate components I didn&amp;rsquo;t want to add more of that, and since there already was a Zookeeper ensemble running - required by one of the services, I&amp;rsquo;ve decided to go that way with my solution. What is Zookeeper?Just a crude distributed synchronization framework.</description>
    </item>
    
    <item>
      <title>Operational problems with Zookeeper</title>
      <link>https://marcin.cylke.com.pl/2013/03/21/zookeeper-tips/</link>
      <pubDate>Thu, 21 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://marcin.cylke.com.pl/2013/03/21/zookeeper-tips/</guid>
      <description>This post is a summary of what has been presented by Kathleen Ting on StrangeLoop conference. You can watch the original here: http://www.infoq.com/presentations/Misconfiguration-ZooKeeper
I&amp;rsquo;ve decided to put this selection here for quick reference.
Connection mismanagement too many connections
WARN [NIOServerCxn.Factory: 0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@247] - Too many connections from /xx.x.xx.xxx - max is 60 running out of ZK connections? set maxClientCnxns=200 in zoo.cfg HBase client leaking connections? fixed in HBASE-3777, HBASE-4773, HBASE-5466 manually close connections connection closes prematurely</description>
    </item>
    
    <item>
      <title>After WHUG meeting</title>
      <link>https://marcin.cylke.com.pl/2012/11/30/after-whug/</link>
      <pubDate>Fri, 30 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>https://marcin.cylke.com.pl/2012/11/30/after-whug/</guid>
      <description>Here are the slides from the talk a gave yesterday. If you have any questions, please ask.</description>
    </item>
    
    <item>
      <title>WHUG 8. Beyond Hadoop - checking other options</title>
      <link>https://marcin.cylke.com.pl/2012/11/26/whug-8-beyond-hadoop/</link>
      <pubDate>Mon, 26 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>https://marcin.cylke.com.pl/2012/11/26/whug-8-beyond-hadoop/</guid>
      <description>W najbliższy czwartek - czyli 29.11.2012 - poprowadzę prezentację w ramach Warsaw Hadoop User Group. Swoją obecność można odklinąć tu http://www.meetup.com/warsaw-hug/
A o czym będę mówił? Przeklejka ze strony WHUG:
Marcin skupi się na współpracy ekosystemu Hadoopa z innymi narzędziami. Pokaże jak prosto i wygodnie przetwarzać grafy i jak stosować podejście Big Data, w czasie rzeczywistym. Poruszy również temat łatwiejszego tworzenia algorytmów Map-Reduce
Będzie to nieco mniej technicza (ale wciąż praktyczna) wycieczka po obrzeżach tematyki, która jest zwykle poruszana w połączeniu z Hadoop-em.</description>
    </item>
    
    <item>
      <title>Hadoop HA setup</title>
      <link>https://marcin.cylke.com.pl/2012/10/30/hadoop-ha/</link>
      <pubDate>Tue, 30 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>https://marcin.cylke.com.pl/2012/10/30/hadoop-ha/</guid>
      <description>&lt;p&gt;With the advent of Hadoop&amp;rsquo;s 2.x version, there finally is a working
High-Availability solution. Even two of those. Now it really is easy to
configure and use those solutions. It no longer require external
components, like
&lt;a href=&#34;http://blog.cloudera.com/blog/2009/07/hadoop-ha-configuration/&#34;&gt;DRBD&lt;/a&gt;.
It all is just neatly packed into Cloudera Hadoop distribution - the
precursor of this solution.&lt;/p&gt;
&lt;p&gt;Read on to find out how to use it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hadoop for Enterprises</title>
      <link>https://marcin.cylke.com.pl/2012/06/18/hadoop-for-enterprises/</link>
      <pubDate>Mon, 18 Jun 2012 09:08:09 +0000</pubDate>
      
      <guid>https://marcin.cylke.com.pl/2012/06/18/hadoop-for-enterprises/</guid>
      <description>Hadoop&#39;s usage as a big data processing framework gains a lot of attention lately. Now, not only big players see, that they can embrace the data their sites or products are generating and develop their businesses on it. For that to happen two things are needed: the data itself and means of processing really big amounts of it. Gathering data is relatively easy. These are not necessarily structured data, you don&#39;t need to plan their usage at first.</description>
    </item>
    
  </channel>
</rss>
